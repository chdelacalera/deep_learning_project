{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Model Transfer Learning for Image Classification**\n",
    "\n",
    "This notebook demonstrates how to use transfer learning with multiple pre-trained models for image classification. We'll compare different models to find the best one for our specific task.\n",
    "\n",
    "## What is Transfer Learning?\n",
    "\n",
    "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. It's particularly useful in deep learning where pre-trained models (such as those trained on ImageNet) can be fine-tuned for specific domains with less data.\n",
    "\n",
    "## Key Benefits:\n",
    "- Reduced training time\n",
    "- Requires less data\n",
    "- Often achieves better performance\n",
    "- Lower computational requirements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Packages\n",
    "\n",
    "First, let's import the necessary packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "# Import from our CNN module\n",
    "from src.cnn import CNN, load_data, load_model_weights\n",
    "\n",
    "# Import from our utils module\n",
    "from src.utils import (\n",
    "    train_one_epoch, validate, train_model_with_early_stopping,\n",
    "    plot_training_history, plot_confusion_matrix, predict_sample_images,\n",
    "    get_pretrained_model\n",
    ")\n",
    "\n",
    "# Import Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Define the experiment parameters and model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common configuration:\n",
      "  batch_size: 32\n",
      "  img_size: 224\n",
      "  epochs: 20\n",
      "  patience: 5\n",
      "\n",
      "Model configurations:\n",
      "  ResNet50: {'base_model': 'resnet50', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'unfreeze_layers': 2}\n",
      "  ResNet18: {'base_model': 'resnet18', 'learning_rate': 0.0005, 'weight_decay': 1e-05, 'unfreeze_layers': 2}\n",
      "  EfficientNet-B0: {'base_model': 'efficientnet_b0', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'unfreeze_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "# Common training parameters\n",
    "common_config = {\n",
    "    \"batch_size\": 32,       # Number of images per batch\n",
    "    \"img_size\": 224,        # Image size for model input\n",
    "    \"epochs\": 20,           # Maximum number of training epochs\n",
    "    \"patience\": 5,          # Early stopping patience\n",
    "}\n",
    "\n",
    "# Model-specific configurations\n",
    "model_configs = {\n",
    "    \"ResNet50\": {\n",
    "        \"base_model\": \"resnet50\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"unfreeze_layers\": 2\n",
    "    },\n",
    "    \"ResNet18\": {\n",
    "        \"base_model\": \"resnet18\",\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"unfreeze_layers\": 2\n",
    "    },\n",
    "    \"EfficientNet-B0\": {\n",
    "        \"base_model\": \"efficientnet_b0\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"unfreeze_layers\": 2\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Common configuration:\")\n",
    "for key, value in common_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nModel configurations:\")\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"  {model_name}: {config}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load the image data from our dataset using the load_data function from our CNN module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 15\n",
      "Training samples: 2985\n",
      "Validation samples: 1500\n",
      "\n",
      "Classes: ['Bedroom', 'Coast', 'Forest', 'Highway', 'Industrial', 'Inside city', 'Kitchen', 'Living room', 'Mountain', 'Office', 'Open country', 'Store', 'Street', 'Suburb', 'Tall building']\n"
     ]
    }
   ],
   "source": [
    "# Path to training and validation data\n",
    "# Update these paths to match your dataset location\n",
    "train_dir = './dataset/training'\n",
    "valid_dir = './dataset/validation'\n",
    "\n",
    "# Load data using the load_data function\n",
    "train_loader, valid_loader, num_classes = load_data(\n",
    "    train_dir, \n",
    "    valid_dir, \n",
    "    batch_size=common_config[\"batch_size\"], \n",
    "    img_size=common_config[\"img_size\"]\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_loader.dataset)}\")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"\\nClasses: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare for Multiple Models\n",
    "\n",
    "Create a dictionary to store results for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results for each model\n",
    "models_results = {}\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print GPU information if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions\n",
    "\n",
    "Define a function to train a single model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model_name, model_config, common_config):\n",
    "    \"\"\"Train a single model with the specified configuration and log to W&B.\"\"\"\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'-'*50}\\n\")\n",
    "    \n",
    "    # 1) Start run of W&B\n",
    "    run = wandb.init(\n",
    "        project=\"deep-lab-models\",\n",
    "        name=f\"run-{model_name}\",\n",
    "        group=\"multi-model-transfer-learning\",\n",
    "        config={**common_config, **model_config}\n",
    "    )\n",
    "\n",
    "    # 2) Obtain and prepare the model\n",
    "    base_model = get_pretrained_model(model_config[\"base_model\"])\n",
    "    print(f\"Using {model_config['base_model']} as base model\")\n",
    "    model = CNN(base_model, num_classes, model_config[\"unfreeze_layers\"]).to(device)\n",
    "\n",
    "    # Show trainable parameters\n",
    "    print(\"\\nTrainable layers:\")\n",
    "    trainable_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  {name}\")\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"\\nTotal trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=model_config[\"learning_rate\"],\n",
    "        weight_decay=model_config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # 3) Training with stopping and logging\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(common_config[\"epochs\"]):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc\n",
    "        })\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # Manual early stopping \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= common_config[\"patience\"]:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # 4) Log \n",
    "    fig_cm = plot_confusion_matrix(val_labels, val_preds, class_names)\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(fig_cm)})\n",
    "\n",
    "    sample_fig = predict_sample_images(model, valid_loader, device, class_names, num_samples=5)\n",
    "    wandb.log({\"sample_predictions\": wandb.Image(sample_fig)})\n",
    "\n",
    "    # 5) Save final model\n",
    "    model_filename = f\"{model_name.lower().replace('-', '_')}_finetune.pt\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    artifact = wandb.Artifact(f\"{model_name}-model\", type=\"model\")\n",
    "    artifact.add_file(model_filename)\n",
    "    run.log_artifact(artifact)\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    val_acc = history[\"val_acc\"][-1]\n",
    "    training_time = None  \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"val_preds\": val_preds,\n",
    "        \"val_labels\": val_labels,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"training_time\": training_time,\n",
    "        \"config\": {**model_config, **common_config}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 1: ResNet50\n",
    "\n",
    "Train our first model: ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Training ResNet50\n",
      "--------------------------------------------------\n",
      "\n",
      "Using resnet50 as base model\n",
      "\n",
      "Trainable layers:\n",
      "  fc.0.weight\n",
      "  fc.0.bias\n",
      "  fc.3.weight\n",
      "  fc.3.bias\n",
      "\n",
      "Total trainable parameters: 2,113,551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train ResNet50\u001b[39;00m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mResNet50\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m models_results[model_name] = \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m      6\u001b[39m plot_training_history(models_results[model_name][\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_single_model\u001b[39m\u001b[34m(model_name, model_config, common_config)\u001b[39m\n\u001b[32m     40\u001b[39m epochs_no_improve = \u001b[32m0\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(common_config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     val_loss, val_acc, val_preds, val_labels = validate(model, valid_loader, criterion, device)\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Log metrics to wandb\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MBD/Parte2/ML2/deep/src/utils.py:33\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     30\u001b[39m model.train()\n\u001b[32m     31\u001b[39m total_loss, correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:229\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    226\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    231\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:268\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/PIL/Image.py:3256\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3253\u001b[39m     fp = io.BytesIO(fp.read())\n\u001b[32m   3254\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3256\u001b[39m prefix = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3258\u001b[39m preinit()\n\u001b[32m   3260\u001b[39m accept_warnings = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train ResNet50\n",
    "model_name = \"ResNet50\"\n",
    "models_results[model_name] = train_single_model(model_name, model_configs[model_name], common_config)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(models_results[model_name][\"history\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    models_results[model_name][\"val_labels\"], \n",
    "    models_results[model_name][\"val_preds\"], \n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Show sample predictions\n",
    "predict_sample_images(\n",
    "    models_results[model_name][\"model\"], \n",
    "    valid_loader, \n",
    "    device, \n",
    "    class_names, \n",
    "    num_samples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: ResNet18\n",
    "\n",
    "Train our second model: ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Training ResNet18\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/elenamartineztorrijos/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 9.72MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resnet18 as base model\n",
      "\n",
      "Trainable layers:\n",
      "  fc.0.weight\n",
      "  fc.0.bias\n",
      "  fc.3.weight\n",
      "  fc.3.bias\n",
      "\n",
      "Total trainable parameters: 540,687\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train ResNet18\u001b[39;00m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mResNet18\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m models_results[model_name] = \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m      6\u001b[39m plot_training_history(models_results[model_name][\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtrain_single_model\u001b[39m\u001b[34m(model_name, model_config, common_config)\u001b[39m\n\u001b[32m     39\u001b[39m epochs_no_improve = \u001b[32m0\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(common_config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     val_loss, val_acc, val_preds, val_labels = validate(model, valid_loader, criterion, device)\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Log metrics to wandb\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MBD/Parte2/ML2/deep/src/utils.py:33\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     30\u001b[39m model.train()\n\u001b[32m     31\u001b[39m total_loss, correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:229\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    226\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    231\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:268\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl_lab/lib/python3.12/site-packages/PIL/Image.py:3256\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3253\u001b[39m     fp = io.BytesIO(fp.read())\n\u001b[32m   3254\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3256\u001b[39m prefix = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3258\u001b[39m preinit()\n\u001b[32m   3260\u001b[39m accept_warnings = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet18\n",
    "model_name = \"ResNet18\"\n",
    "models_results[model_name] = train_single_model(model_name, model_configs[model_name], common_config)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(models_results[model_name][\"history\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    models_results[model_name][\"val_labels\"], \n",
    "    models_results[model_name][\"val_preds\"], \n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Show sample predictions\n",
    "predict_sample_images(\n",
    "    models_results[model_name][\"model\"], \n",
    "    valid_loader, \n",
    "    device, \n",
    "    class_names, \n",
    "    num_samples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 3: EfficientNet-B0\n",
    "\n",
    "Train our third model: EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet-B0\n",
    "model_name = \"EfficientNet-B0\"\n",
    "models_results[model_name] = train_single_model(model_name, model_configs[model_name], common_config)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(models_results[model_name][\"history\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    models_results[model_name][\"val_labels\"], \n",
    "    models_results[model_name][\"val_preds\"], \n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Show sample predictions\n",
    "predict_sample_images(\n",
    "    models_results[model_name][\"model\"], \n",
    "    valid_loader, \n",
    "    device, \n",
    "    class_names, \n",
    "    num_samples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Select the Best Model\n",
    "\n",
    "Based on the comparison, select the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model with the highest validation accuracy\n",
    "best_model_name = max(models_results.items(), key=lambda x: x[1]['val_acc'])[0]\n",
    "best_model = models_results[best_model_name]['model']\n",
    "best_val_acc = models_results[best_model_name]['val_acc']\n",
    "\n",
    "print(f\"Best model: {best_model_name} with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Save the best model with a special name\n",
    "best_model_filename = f\"best_model_{best_model_name.lower().replace('-', '_')}\"\n",
    "best_model.save_model(best_model_filename)\n",
    "print(f\"Best model saved as {best_model_filename}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation\n",
    "\n",
    "Evaluate the best model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions from the best model\n",
    "print(f\"Showing predictions from the best model: {best_model_name}\")\n",
    "predict_sample_images(best_model, valid_loader, device, class_names, num_samples=8)\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "plot_confusion_matrix(\n",
    "    models_results[best_model_name][\"val_labels\"],\n",
    "    models_results[best_model_name][\"val_preds\"],\n",
    "    class_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
